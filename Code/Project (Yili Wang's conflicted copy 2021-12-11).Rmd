---
title: "Project"
author: "Jinhao Wang"
date: "12/10/2021"
output: pdf_document
---

```{r,warning=F}
##########Clean up and import package:

rm(list=ls(all=TRUE))  #same to clear all in stata
cat("\014")

#x<-c("DT","ggplot2","ggpubr","tidyverse","dplyr","DescTools","PropCIs","qpcR","scales","kableExtra","broom","logistf","ggbiplot","factoextra","psych","reshape2","caret","e1071","rpart","rpart.plot","glmnet","xgboost","Ckmeans.1d.dp")

# new.packages<-x[!(x %in% installed.packages()[,"Package"])]
# if(length(new.packages)) install.packages(new.packages)


lapply(x, require, character.only=T)
coalesce <- function(...) {
  apply(cbind(...), 1, function(x) {
    x[which(!is.na(x))[1]]
  })
}
 library(ggbiplot)
 library(factoextra)
 library(psych)
 library(reshape2)
 library(caret)
 library(e1071)
 library(rpart)
 library(rpart.plot)
 library(caret)
 library(glmnet)
 library(xgboost) # the main algorithm # for the sample dataset 
 library(Ckmeans.1d.dp) # for xgb.ggplot.importance
 library(dplyr)
 suppressMessages(library(shiny))
 suppressMessages(library(DT))
 suppressMessages(library(ggplot2))
 suppressMessages(library(ggpubr))
 suppressMessages(library(tidyverse))
 suppressMessages(library(dplyr))
 suppressMessages(library(DescTools))
 suppressMessages(library(PropCIs))
 suppressMessages(library(qpcR))
 suppressMessages(library(scales))
 suppressMessages(library(kableExtra))
 suppressMessages(library(broom))
 suppressMessages(library(logistf))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(root.dir = normalizePath("C:/Users/wjhlang/Downloads"))
```

```{r}
#data <- read.csv('AllData.csv',header = T) # Single quoted header
dataRDS <- readRDS("/Users/ellie/Dropbox (University of Michigan)/Final/625_Group5/625_Group5/AllData.RDS")
#dataRDS <- as.matrix(dataRDS)
```

## Data Cleaning
```{r}
dataRDS$DIQ010 = ifelse(dataRDS$DIQ010 %in% c(7,9), NA, dataRDS$DIQ010)
dataRDS = dataRDS[!is.na(dataRDS$DIQ010),]
dataRDS$DIQ010 = ifelse(dataRDS$DIQ010 == 3, 1, dataRDS$DIQ010)
table(dataRDS$DIQ010) # Extremely Unbalanced
dataRDS$DIQ010 = as.factor(dataRDS$DIQ010)


# na = colSums(is.na(dataRDS))/nrow(dataRDS)*100
# na=na[na<30]
# data=dataRDS[complete.cases(dataRDS[,names(na)]),names(na)]

Ylist=levels(as.factor(dataRDS$year))
for(year in Ylist){
  data=dataRDS[dataRDS$year==year,]
  na = colSums(is.na(data))/nrow(data)*100
  na=na[na<10]
  data=data[complete.cases(data[,names(na)]),names(na)]
  
  print(year)
  print(table(data$DIQ010))
  
  #Loop for factor names:
  varname=c()
  for(varn in names(na)){
    if(length(levels(as.factor(data[[varn]])))==1){
      varname=c(varname,varn)
    }
  }
  print(varname)
  data=data[,!colnames(data) %in% varname]
  
  assign(paste0("data",year),data)
}



```

## Outlier Detection
```{r}
fit.lm <- lm(as.numeric(DIQ010)~., data = data1999_2000)
#calculate cook's distance
cooksd = cooks.distance(fit.lm)
#get the outliers table
dfOutliers<-head(data1999_2000[cooksd > 4 * mean(cooksd, na.rm=T), ])%>%data.frame()
kable(dfOutliers,caption = "Outliers")%>%
 kable_styling(latex_options="scale_down")%>%kable_styling(latex_options = "hold_position")%>%
 kable_classic(full_width = F, html_font = "Cambria")
#outliers visualization
plot(cooksd, pch = 20, cex = 1, main = "Outlier",col = brewer.pal(9,"Greys"))
abline(h = 4*mean(cooksd, na.rm = T), col = "steelblue")
#outlier deletion
data <- data[cooksd <= 4 * mean(cooksd, na.rm=T), ]
```

## Principal Component Analysis
```{r}
library(ggplot2)
library(ggbiplot)
library(gridExtra)
library(factoextra)
library(psych)
library(reshape2)
fit.pca = prcomp(data1999_2000[,-149],scale. = TRUE)

knitr::kable(fit.pca$rotation %>% round(3),
 caption = "Variable importance",booktabs = T)%>%
 kable_styling(latex_options="scale_down")%>%kable_styling(latex_options = "hold_position")%>%
  kable_classic(full_width = F, html_font = "Cambria")
ggbi<- ggbiplot(fit.pca, obs.scale = 1, var.scale = 1,
 ellipse = TRUE, circle = TRUE) +
 scale_color_discrete(name = '') +
 theme(legend.direction = 'horizontal', legend.position = 'top')
screePlot<-fviz_eig(fit.pca, addlabels = TRUE);screePlot
# Visualize variable with cos2 >= 0.6
# Top 10 active variables with the highest cos2
highestCos2<-fviz_pca_var(fit.pca, select.var= list(cos2 = 11), repel=T,
 col.var = "contrib")
grid.arrange(ggbi,screePlot,nrow=1,top = "Visualizaiton of PCA")
grid.arrange(highestCos2,nrow=1,top="Contribution of Variables")
# cumulative variance
s <- summary(fit.pca)
cuv <- s$importance[3,]
cv <- data.frame(cuv)
r <- matrix(cv[,1], ncol = 25)
df.cuv <- data.frame(r)
colnames(df.cuv) <- rownames(cv)[1:25]
kable(df.cuv %>% round(3),
 caption = "Cumulative Variance")%>%
 kable_classic(full_width = F, html_font = "Cambria")%>% footnote(alphabet = "Table 2")

```

## Random Forest
```{r}
library(caret)
library(randomForest)
set.seed(1)
# 80% as training data; 20% as testing
train.index <- createDataPartition(data1999_2000$DIQ010, p = 0.8, list= FALSE)
train.data <- data1999_2000[train.index ,]
test.data <- data1999_2000[-train.index,]
Varibale.Importance =randomForest(DIQ010~., data = train.data,ntree = 2000,
mtry = 19,importance = TRUE)
rf.pre = predict(Varibale.Importance, test.data,type="class")
#confusion matrix
rp <- predict(Varibale.Importance, train.data,type="class")
ft <- confusionMatrix(rp,train.data$DIQ010)
cm.f <- confusionMatrix(rf.pre,test.data$DIQ010);cm.f
varImpPlot(Varibale.Importance)
```

## SVM
```{r}
#library(e1071)
fit.svm = svm(class~., train.data,
 probability = TRUE,cost = 26)
pre.svm = predict(fit.svm, test.data,
 decision.values = TRUE, probability = TRUE)
#confusion table
sp <- predict(fit.svm, train.data,
 decision.values = TRUE, probability = TRUE)
sm <- confusionMatrix(sp,train.data$class)
cm.s<- confusionMatrix(pre.svm,test.data$class);cm.s
```

## Decision Tree
```{r}
# library(rpart)
# library(rpart.plot)
fit.dt <- rpart(class~., train.data,cp=.02)
rpart.plot(fit.dt)
#Model performance
dt.pre = predict(fit.dt, newdata=test.data,type="class")
cm.dt<- confusionMatrix(dt.pre,test.data$class);cm.dt
dp <- predict(fit.dt, newdata=train.data,type="class")
cd <- confusionMatrix(dp,train.data$class)
#variable importance
df3 = data.frame(Variable.importance=fit.dt$variable.importance)
knitr::kable(df3 %>% round(3),
 caption = "Variable Importance")%>%kable_styling(latex_options = "hold_position")
```

## Lasso Regression
```{r}
# library(caret)
# library(glmnet)
# Dummy code categorical predictor variables
x <- model.matrix(class~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- ifelse(train.data$class == "ckd", 1, 0)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
 lambda = cv.lasso$lambda.min)
# Display regression coefficients
coef(model)
x.test <- model.matrix(class ~., test.data)[,-1]
probabilities <- model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "ckd", "notckd")
# Model accuracy
observed.classes <- test.data$class
mean(predicted.classes == observed.classes)
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
coef(cv.lasso, cv.lasso$lambda.min)
coef(cv.lasso, cv.lasso$lambda.1se)
## Lasso min
# Final model with lambda.min
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
 lambda = cv.lasso$lambda.min)
# Make prediction on test data
x.test <- model.matrix(class ~., test.data)[,-1]
probabilities <- lasso.model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "ckd", "notckd")
# Model accuracy
cm <- confusionMatrix(factor(predicted.classes),test.data$class);cm
tLL <- lasso.model$nulldev - deviance(lasso.model)
k <- lasso.model$df
n <- lasso.model$nobs
AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)
AICc
## Lasso 1se
# Final model with lambda.mse
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
 lambda = cv.lasso$lambda.1se)
# Make prediction on test data
x.test <- model.matrix(class ~., test.data)[,-1]
probabilities <- lasso.model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "ckd", "notckd")
# Model accuracy
cm.k <- confusionMatrix(factor(predicted.classes),test.data$class);cm.k
tLL <- lasso.model$nulldev - deviance(lasso.model)
k <- lasso.model$df
n <- lasso.model$nobs
AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)
```

## XGBoost
```{r}
#using xghboost with ckd(contain NA)
# library(xgboost) # the main algorithm # for the sample dataset 
# library(Ckmeans.1d.dp) # for xgb.ggplot.importance
# library(dplyr)
ckd.num <- data.frame(lapply(ckd.rearrange, as.numeric))
class <- ckd.num$class
label <- as.integer(ckd.num$class)-1
ckd.num$class = NULL
train.index <- sample(nrow(ckd.num),floor(0.8*nrow(ckd.num)))
train.data = as.matrix(ckd.num[train.index,])
train.label = label[train.index]
test.data = as.matrix(ckd.num[-train.index,])
test.label = label[-train.index]
#set matrix
trainMatrix = xgb.DMatrix(data=train.data,label=train.label)
testMatrix = xgb.DMatrix(data=test.data,label=test.label)
# Define the parameters
params = list(
 booster="gbtree",
 eta=0.001,
 max_depth=5,
 gamma=3,
 subsample=0.75,
 colsample_bytree=1,
 objective="binary:logistic",
 eval_metric="rmse"
)
# Train the XGBoost classifier
fit.xg=xgb.train(
 params=params,
 data=trainMatrix,
 nrounds=1000,
 
 early_stopping_rounds=10,
 watchlist=list(training=trainMatrix,testing=testMatrix),
 verbose=0
)
XGpred <- predict(fit.xg, newdata = testMatrix)
XGprediction <- as.numeric(XGpred>0.5)
# Use the predicted label with the highest probability
cmXG<-confusionMatrix(factor(XGprediction),
 factor(test.label));cmXG
# get the feature real names
names <- colnames(ckd.num)
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = fit.xg)
#overfitting check
oc <- fit.xg$evaluation_log[c(1:5, 91:95, 995:1000)]
oc%>%data.frame()%>%
kable(caption = "Overfitting Check")%>%
 kable_styling(latex_options="scale_down")%>%kable_styling(latex_options = "hold_position")%>%
 kable_classic(full_width = F, html_font = "Cambria")
#variable importance plot
# plot
n.col <- 26
my.col2 <- colorRampPalette(brewer.pal(9,"Set2"))(n.col)
xgb.ggplot.importance(importance_matrix)+scale_fill_manual(values = my.col2)+theme_classic()

```

## Logisitic Regression
```{r}
lmod <- glm(class~.,data = train, family="binomial",maxit=50)
summary(lmod)
car::vif(lmod)
newmod <- step(lmod)
summary(newmod)
```